{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "68f69423",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scikitplot'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#>1#(71524626):(71524626)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscikitplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mskplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#<1#(71524626)~%(448899864) #>1#(85844285):(85844285)\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikitplot'"
          ]
        }
      ],
      "source": [
        "#>1#(71524626):(71524626)\n",
        "import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "#<1#(71524626)~%(448899864) #>1#(85844285):(85844285)\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "#<1#(85844285)~%(-1952786418) #>1#(33299708):(33299708)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout\n",
        "#<1#(33299708)~%(533338258) #>1#(16183209):(16183209)\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#<1#(16183209)~%(696278483)\n",
        "# Leave for imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76da3b5c",
      "metadata": {},
      "source": [
        "Run the following !pip3 cells to install scikit-plot and tensorflow which are needed for this sandbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b5cbac",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install scikit-plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd3374c",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df6e449",
      "metadata": {},
      "source": [
        "Welcome to automating neural networks with Arctic Fox! Our focus of these automations is to make developing AI / ML solutions more akin to how we think about AI / ML - get data, build the network, train the model, and view the results. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69706a23",
      "metadata": {},
      "source": [
        "# Automations with Nueral Network Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307d8ec9",
      "metadata": {},
      "source": [
        "We all love AI! Well, most of us... And it's amazing how much easier it is to develop AI solutions today than in the past. But, what we found, is that a lot of times when we take on a new project, try to recycle an old notebook, etc. there's always more edits / refactoring than we thought. So, we figured it would be easier if a lot of it was automated!\n",
        "\n",
        "To do this, we created 4 automations - Data, NeuralNetwork (NN), Train, and Visualize. We placed them each below, with some description, and pre-populated some parameters that worked for us. Play around with it and enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e124b71",
      "metadata": {},
      "source": [
        "##### Note\n",
        "The examples in codespace use simple csvs and neural networks due to processing power constraints. Automations exist for convolutional neural networks as well. However, for non-gpu environments, training is too slow to show meaningful examples. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb546ac",
      "metadata": {},
      "source": [
        "### Data, Load CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4903c227",
      "metadata": {},
      "source": [
        "The first automation will load in the data. In this case, our source is a csv. Other times, the source could be a library in an imported source, a collection of images, etc. Additionally, we can tell it what to predict - we could tell it the name of a column or the column index. Our example uses the popular wine dataset, and since there are no headers, we give it the column index. \n",
        "\n",
        "Additionally, by providing column index 0, Arctic Fox will look at the data and determine that the desired model is a classifier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43279c7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#[Data wine.csv --predict 0]#@16183209 #>1#(16183209):(16183209)\n",
        "wine = pd.read_csv('wine.csv')\n",
        "wineAllFeatures = wine.copy()\n",
        "\n",
        "column0ToNumber = {\n",
        "    1: 0, 2: 1, 3: 2\n",
        "}\n",
        "\n",
        "wineAllFeatures.iloc[:,0] = wineAllFeatures.iloc[:,0].apply(lambda cell : column0ToNumber[cell])\n",
        "\n",
        "wineAllLabels = wineAllFeatures.iloc[:,0]\n",
        "wineAllFeatures.drop(wineAllFeatures.columns[0], axis=1, inplace=True)\n",
        "wineAllFeatures = np.array(wineAllFeatures)\n",
        "\n",
        "\n",
        "numberToWineAllLabels = {\n",
        "    0: 1, 1: 2, 2: 3\n",
        "}\n",
        "\n",
        "wineTrainingFeatures, wineTestFeatures, wineTrainingLabels, wineTestLabels = train_test_split(wineAllFeatures, wineAllLabels, test_size=0.2)\n",
        "\n",
        "wineTrainingLabels = tf.one_hot(wineTrainingLabels, 3)\n",
        "wineTestLabels = tf.one_hot(wineTestLabels, 3)\n",
        "#<1#(16183209)~%(352366057)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27183f87",
      "metadata": {},
      "source": [
        "### Build Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b4d74df",
      "metadata": {},
      "source": [
        "Next, we use the NeuralNetwork automation, or NN for short. We have provided some example parameters, but if you didn't have these, you could always just use #[NN --help] to see the available options. The start and end specify the number of nodes in the start and end (second to last) layers. DenseLayers specifies how many layers should be in the model. \n",
        "\n",
        "Play around with the parameters to see how the model changes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2323d442",
      "metadata": {},
      "outputs": [],
      "source": [
        "#[NN --denseLayers 20 --denseStart 96 --denseEnd 32 --dropOutRatio 1:5]#@33299708 #>1#(33299708):(33299708)\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(96, activation='relu', input_shape=(13,)))\n",
        "model.add(Dense(90, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "model.add(Dense(79, activation='relu'))\n",
        "model.add(Dense(75, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(70, activation='relu'))\n",
        "model.add(Dense(66, activation='relu'))\n",
        "model.add(Dense(62, activation='relu'))\n",
        "model.add(Dense(58, activation='relu'))\n",
        "model.add(Dense(55, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(52, activation='relu'))\n",
        "model.add(Dense(49, activation='relu'))\n",
        "model.add(Dense(46, activation='relu'))\n",
        "model.add(Dense(43, activation='relu'))\n",
        "model.add(Dense(40, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(38, activation='relu'))\n",
        "model.add(Dense(36, activation='relu'))\n",
        "model.add(Dense(34, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "#<1#(33299708)~%(-543925969)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57484240",
      "metadata": {},
      "outputs": [],
      "source": [
        "#>1#(33299708):(33299708)\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(),\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "#<1#(33299708)~%(277325532)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dc1b38",
      "metadata": {},
      "outputs": [],
      "source": [
        "#>1#(33299708):(33299708)\n",
        "model.summary()\n",
        "#<1#(33299708)~%(1975814818)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5da953",
      "metadata": {},
      "source": [
        "### Train Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299648e7",
      "metadata": {},
      "source": [
        "Pretty much the only thing to specify when training is how long to train, or the number of epochs. You can always re-run the training, or fit, cell to do more training. Train, retrain, keep training, that's all pretty normal. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1c6f5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "#[Train --epochs 300]#@85844285 #>1#(85844285):(85844285)\n",
        "# Fit data to model\n",
        "history = model.fit(\n",
        "    wineTrainingFeatures,\n",
        "    wineTrainingLabels,\n",
        "    batch_size=1,\n",
        "    epochs=300,\n",
        "    verbose=1,\n",
        "    validation_split=0.2\n",
        ")\n",
        "#<1#(85844285)~%(-1821562651)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5369bab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#>1#(85844285):(85844285)\n",
        "# Generate generalization metrics\n",
        "score = model.evaluate(wineTestFeatures, wineTestLabels, verbose=0, batch_size=1)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "#<1#(85844285)~%(-1738228711)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6433913d",
      "metadata": {},
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712c41d6",
      "metadata": {},
      "source": [
        "Finally, it's always import to visualize a model's performance. Seeing the training loss and accuracy evolve over time let's you get a feel fore how the model trained. We think for a classifier that the confusion matrix is one of the most telling visuals. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f3d008",
      "metadata": {},
      "outputs": [],
      "source": [
        "#>1#(71524626):(71524626)\n",
        "#***Info: Generating graph code for the accuracy and loss for training and validation results\n",
        "#<1#(71524626)~%(0)\n",
        "#[Visualize --loss --accuracy --confusionMatrix]#@71524626 #>1#(71524626):(71524626)\n",
        "#***Plot history: Accuracy\n",
        "plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='val')\n",
        "plt.title('Train and Validation Accuracy History')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#<1#(71524626)~%(126940084)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f022ff94",
      "metadata": {},
      "outputs": [],
      "source": [
        "#>1#(71524626):(71524626)\n",
        "#***Plot history: Loss\n",
        "plt.plot(history.history['loss'], color='blue', label='train')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='val')\n",
        "plt.title('Train and Validation Loss History')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#<1#(71524626)~%(-52552)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17824830",
      "metadata": {},
      "outputs": [],
      "source": [
        "#>1#(71524626):(71524626)\n",
        "#***Visualize Confusion Matrix for Test Data\n",
        "confusionMatrixPrediction = model.predict(wineTestFeatures, batch_size=1)\n",
        "confusionMatrixActual = [np.argmax(label) for label in wineTestLabels]\n",
        "confusionMatrixPrediction = [np.argmax(label) for label in confusionMatrixPrediction]\n",
        "\n",
        "confusionMatrixActual = [numberToWineAllLabels[label] for label in confusionMatrixActual]\n",
        "confusionMatrixPrediction = [numberToWineAllLabels[label] for label in confusionMatrixPrediction]\n",
        "\n",
        "skplt.metrics.plot_confusion_matrix(confusionMatrixActual, confusionMatrixPrediction, normalize=False, title = 'Confusion Matrix for wine.csv')\n",
        "skplt.metrics.plot_confusion_matrix(confusionMatrixActual, confusionMatrixPrediction, normalize=True, title = 'Normalized Confusion Matrix for wine.csv')\n",
        "#<1#(71524626)~%(1746290697)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "468d5296",
      "metadata": {},
      "source": [
        "That's a wrap on the basics of automating neural networks. The next sandbox makes a small change, and instantly your notebook is setup for regression. Check it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c918d6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
